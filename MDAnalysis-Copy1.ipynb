{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MD analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "%matplotlib inline\n",
    "import mdtraj as md\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.cluster.hierarchy\n",
    "from scipy.spatial.distance import squareform\n",
    "import pandas as pd\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import MDAnalysis as mda\n",
    "from statistics import mode\n",
    "import ast\n",
    "import msmexplorer as msme\n",
    "from msmbuilder.utils import load,dump\n",
    "import itertools\n",
    "from msmbuilder.featurizer import ContactFeaturizer\n",
    "from msmbuilder.featurizer import DihedralFeaturizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#XRD Ensemble\n",
    "#28 4NPQ\n",
    "#18 4HFI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Default values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_location = '../pdc/'\n",
    "ensemble_location = '/home/scottzhuang/masterthesis/miscellanies/pdb_ensemble/'\n",
    "default_skip = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_notes = ['5NJY_pH70_md1','5NJY_pH70_md2','5NJY_pH70_md3','5NJY_F238L_pH70_md1',\n",
    "                 '5NJY_F238L_pH70_md2','5NJY_F238L_pH70_md3','5NJY_I233T_pH70_md4',\n",
    "                  '5NJY_I233T_pH70_md2','5NJY_I233T_pH70_md3','5NJY_F238LI233T_pH70_md1',\n",
    "                  '5NJY_F238LI233T_pH70_md2','5NJY_F238LI233T_pH70_md3','5NJY_pH46_md1','5NJY_pH46_md2','5NJY_pH46_md3','5NJY_F238L_pH46_md1',\n",
    "                 '5NJY_F238L_pH46_md2','5NJY_F238L_pH46_md3','5NJY_I233T_pH46_md1',\n",
    "                  '5NJY_I233T_pH46_md2','5NJY_I233T_pH46_md3','5NJY_F238LI233T_pH46_md1',\n",
    "                  '5NJY_F238LI233T_pH46_md2','5NJY_F238LI233T_pH46_md3','4HFI_pH46_md1','4HFI_pH46_md2','4HFI_pH46_md3','4HFI_F238L_pH46_md1',\n",
    "                 '4HFI_F238L_pH46_md2','4HFI_F238L_pH46_md3','4HFI_I233T_pH46_md1',\n",
    "                  '4HFI_I233T_pH46_md2','4HFI_I233T_pH46_md3','4HFI_F238LI233T_pH46_md1',\n",
    "                  '4HFI_F238LI233T_pH46_md2','4HFI_F238LI233T_pH46_md3','4NPQ_pH70_md5','4NPQ_pH70_md6','4NPQ_pH70_md7','4NPQ_F238L_pH70_md3',\n",
    "                 '4NPQ_F238L_pH70_md4','4NPQ_F238L_pH70_md5','4NPQ_I233T_pH70_md3',\n",
    "                  '4NPQ_I233T_pH70_md4','4NPQ_I233T_pH70_md5','4NPQ_F238LI233T_pH70_md3',\n",
    "                  '4NPQ_F238LI233T_pH70_md4','4NPQ_F238LI233T_pH70_md5','4HFI_F238A_pH46_md1',\n",
    "                 '4HFI_F238A_pH46_md2','4HFI_F238A_pH46_md3','4HFI_F238L_pH46_md4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_notess = [['5NJY_pH70_md1','5NJY_pH70_md2','5NJY_pH70_md3'],['5NJY_F238L_pH70_md1',\n",
    "                 '5NJY_F238L_pH70_md2','5NJY_F238L_pH70_md3'],['5NJY_I233T_pH70_md4',\n",
    "                  '5NJY_I233T_pH70_md2','5NJY_I233T_pH70_md3'],['5NJY_F238LI233T_pH70_md1',\n",
    "                  '5NJY_F238LI233T_pH70_md2','5NJY_F238LI233T_pH70_md3'],['5NJY_pH46_md1','5NJY_pH46_md2','5NJY_pH46_md3'],['5NJY_F238L_pH46_md1',\n",
    "                 '5NJY_F238L_pH46_md2','5NJY_F238L_pH46_md3'],['5NJY_I233T_pH46_md1',\n",
    "                  '5NJY_I233T_pH46_md2','5NJY_I233T_pH46_md3'],['5NJY_F238LI233T_pH46_md1',\n",
    "                  '5NJY_F238LI233T_pH46_md2','5NJY_F238LI233T_pH46_md3'],['4HFI_pH46_md1','4HFI_pH46_md2','4HFI_pH46_md3'],['4HFI_F238L_pH46_md1',\n",
    "                 '4HFI_F238L_pH46_md2','4HFI_F238L_pH46_md3'],['4HFI_I233T_pH46_md1',\n",
    "                  '4HFI_I233T_pH46_md2','4HFI_I233T_pH46_md3'],['4HFI_F238LI233T_pH46_md1',\n",
    "                  '4HFI_F238LI233T_pH46_md2','4HFI_F238LI233T_pH46_md3'],['4NPQ_pH70_md5','4NPQ_pH70_md6','4NPQ_pH70_md7'],['4NPQ_F238L_pH70_md3',\n",
    "                 '4NPQ_F238L_pH70_md4','4NPQ_F238L_pH70_md5'],['4NPQ_I233T_pH70_md3',\n",
    "                  '4NPQ_I233T_pH70_md4','4NPQ_I233T_pH70_md5'],['4NPQ_F238LI233T_pH70_md3',\n",
    "                  '4NPQ_F238LI233T_pH70_md4','4NPQ_F238LI233T_pH70_md5'],['4HFI_F238A_pH46_md1',\n",
    "                 '4HFI_F238A_pH46_md2','4HFI_F238A_pH46_md3'],['4HFI_F238L_pH46_md4']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "1. Fix periodic boundary conditions (PBC) & choose a suitable timestep.\n",
    "2. transform gro file into pdb file with chain information recovered.\n",
    "3. create metadata\n",
    "4. calculate RMSD, RMSF, PCA... & cluster\n",
    "5. extract gating features & hydration data\n",
    "6. extract HBond information of TMD\n",
    "7. plotting Hbond mapping & elucidate important interactions\n",
    "8. geometric features (dihedral angle...)\n",
    "9. analysis data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step1\n",
    "using script **pre_fixing.sh**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_chainid_ca(traj_note):\n",
    "    location = default_location\n",
    "    skip = default_skip\n",
    "    top_location = location + traj_note + '/' + traj_note + \".ca.gro\"\n",
    "    traj = md.load(top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    for i in range (0,5):\n",
    "        table.loc[311*i:311*i + 311,'chainID'] = i\n",
    "    t2 = md.Topology.from_dataframe(table, bonds)\n",
    "    traj.topology = t2\n",
    "    traj.save_pdb(location + traj_note + '/' + traj_note +'.ca.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_chainid_protein(traj_note):\n",
    "    location = default_location\n",
    "    skip = default_skip\n",
    "    top_location = location + traj_note + '/' + traj_note + \".protein.gro\"\n",
    "    traj = md.load(top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    chain_start_index = list(table[(table['resSeq'] == 5) & (table['name'] == 'MN1')].index)\n",
    "    chain_end_index = list(table[(table['resSeq'] == 315) & (table['name'] == 'OXT')].index)\n",
    "    for i in range (0,5):\n",
    "        table.loc[chain_start_index[i]:chain_end_index[i]+1,'chainID'] = i\n",
    "    table.loc[chain_end_index[i]+1:,'chainID'] = 5\n",
    "    t2 = md.Topology.from_dataframe(table, bonds)\n",
    "    traj.topology = t2\n",
    "    traj.save_pdb(location + traj_note + '/' + traj_note +'.protein.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_chainid_system(traj_note):\n",
    "    location = default_location\n",
    "    skip = default_skip\n",
    "    top_location = location + traj_note + '/' + traj_note + \".system.gro\"\n",
    "    traj = md.load(top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    chain_start_index = list(table[(table['resSeq'] == 5) & (table['name'] == 'MN1')].index)\n",
    "    chain_end_index = list(table[(table['resSeq'] == 315) & (table['name'] == 'OXT')].index)\n",
    "    for i in range (0,5):\n",
    "        table.loc[chain_start_index[i]:chain_end_index[i]+1,'chainID'] = i\n",
    "    table.loc[chain_end_index[i]+1:,'chainID'] = 5\n",
    "    table.loc[table.resName == 'HOH','resSeq'] = 1000\n",
    "    table.loc[table.resName == 'HOH','chainID'] = 6\n",
    "    t2 = md.Topology.from_dataframe(table, bonds)\n",
    "    traj.topology = t2\n",
    "    traj.save_pdb(location + traj_note + '/' + traj_note +'.system.pdb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_twosubunit_protein(traj_note):\n",
    "    location = default_location\n",
    "    skip = default_skip\n",
    "    top_location = location + traj_note + '/' + traj_note + \".protein.gro\"\n",
    "    xtc_location = location + traj_note + '/' + traj_note + \".skip10.protein.xtc\"\n",
    "    traj = md.load(xtc_location, top=top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    chain_start_index = list(table[(table['resSeq'] == 5) & (table['name'] == 'MN1')].index)\n",
    "    chain_end_index = list(table[(table['resSeq'] == 315) & (table['name'] == 'OXT')].index)\n",
    "    for i in range (0,5):\n",
    "        table.loc[chain_start_index[i]:chain_end_index[i]+1,'chainID'] = i\n",
    "    table.loc[chain_end_index[i]+1:,'chainID'] = 5\n",
    "    t2 = md.Topology.from_dataframe(table, bonds)\n",
    "    traj.topology = t2\n",
    "    topology = traj.topology\n",
    "\n",
    "    traj01 = traj.atom_slice(topology.select('(chainid 0) or (chainid 1) and backhone'))\n",
    "    \n",
    "    traj12 = traj.atom_slice(topology.select('(chainid 1) or (chainid 2) and backhone'))\n",
    "    traj12.superpose(traj01,0)\n",
    "    \n",
    "    traj23 = traj.atom_slice(topology.select('(chainid 2) or (chainid 3) and backhone'))\n",
    "    traj23.superpose(traj01,0)\n",
    "\n",
    "    traj34 = traj.atom_slice(topology.select('(chainid 3) or (chainid 4) and backhone'))\n",
    "    traj34.superpose(traj01,0)\n",
    "\n",
    "    traj40 = traj.atom_slice(topology.select('(chainid 4) or (chainid 0) and backhone'))\n",
    "    traj40.superpose(traj01,0)\n",
    "\n",
    "    \n",
    "#    traj01.save_pdb(location + traj_note + '/' + traj_note +'.protein.pdb')\n",
    "    traj01[0].save_pdb(location + 'msm/intersubunit/' + traj_note + '.intersubunit.pdb')\n",
    "    traj01.save_xtc(location + 'msm/intersubunit/' + traj_note + '.intersubunit_01.xtc')\n",
    "    traj12.save_xtc(location + 'msm/intersubunit/' + traj_note + '.intersubunit_12.xtc')\n",
    "    traj23.save_xtc(location + 'msm/intersubunit/' + traj_note + '.intersubunit_23.xtc')\n",
    "    traj34.save_xtc(location + 'msm/intersubunit/' + traj_note + '.intersubunit_34.xtc')\n",
    "    traj40.save_xtc(location + 'msm/intersubunit/' + traj_note + '.intersubunit_40.xtc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_notes = ['5NJY_pH46_md1','5NJY_pH46_md2','5NJY_pH46_md3','4HFI_pH46_md1','4HFI_pH46_md2','4HFI_pH46_md3','4NPQ_WT_pH46_md1','4NPQ_BA2_pH46_md1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_twosubunit_protein(traj_note):\n",
    "    location = default_location\n",
    "    skip = default_skip\n",
    "    top_location = '/home/scottzhuang/pdc/msm/' + traj_note + \".protein.gro\"\n",
    "    xtc_location = '/home/scottzhuang/pdc/msm/' + traj_note + \".protein.xtc\"\n",
    "    traj = md.load(xtc_location, top=top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    chain_start_index = list(table[(table['resSeq'] == 5) & (table['name'] == 'MN1')].index)\n",
    "    chain_end_index = list(table[(table['resSeq'] == 315) & (table['name'] == 'OXT')].index)\n",
    "    for i in range (0,5):\n",
    "        table.loc[chain_start_index[i]:chain_end_index[i]+1,'chainID'] = i\n",
    "    table.loc[chain_end_index[i]+1:,'chainID'] = 5\n",
    "    t2 = md.Topology.from_dataframe(table, bonds)\n",
    "    traj.topology = t2\n",
    "    topology = traj.topology\n",
    "\n",
    "    traj01 = traj.atom_slice(topology.select('(chainid 0) or (chainid 1) and backbone'))\n",
    "    \n",
    "    traj12 = traj.atom_slice(topology.select('(chainid 1) or (chainid 2) and backbone'))\n",
    "    traj12.superpose(traj01,0)\n",
    "    \n",
    "    traj23 = traj.atom_slice(topology.select('(chainid 2) or (chainid 3) and backbone'))\n",
    "    traj23.superpose(traj01,0)\n",
    "\n",
    "    traj34 = traj.atom_slice(topology.select('(chainid 3) or (chainid 4) and backbone'))\n",
    "    traj34.superpose(traj01,0)\n",
    "\n",
    "    traj40 = traj.atom_slice(topology.select('(chainid 4) or (chainid 0) and backbone'))\n",
    "    traj40.superpose(traj01,0)\n",
    "\n",
    "    \n",
    "#    traj01.save_pdb(location + traj_note + '/' + traj_note +'.protein.pdb')\n",
    "    traj01[0].save_pdb('/home/scottzhuang/pdc/msm/intersubunit/' + traj_note + '.intersubunit.pdb')\n",
    "    traj01.save_xtc('/home/scottzhuang/pdc/msm/intersubunit/' + traj_note + '.intersubunit_01.xtc')\n",
    "    traj12.save_xtc('/home/scottzhuang/pdc/msm/intersubunit/' + traj_note + '.intersubunit_12.xtc')\n",
    "    traj23.save_xtc('/home/scottzhuang/pdc/msm/intersubunit/' + traj_note + '.intersubunit_23.xtc')\n",
    "    traj34.save_xtc('/home/scottzhuang/pdc/msm/intersubunit/' + traj_note + '.intersubunit_34.xtc')\n",
    "    traj40.save_xtc('/home/scottzhuang/pdc/msm/intersubunit/' + traj_note + '.intersubunit_40.xtc')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "Parallel(n_jobs=num_cores)(delayed(extract_twosubunit_protein)(traj_note) for traj_note in traj_notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "Parallel(n_jobs=num_cores)(delayed(recover_chainid_ca)(traj_note) for traj_note in traj_notes)\n",
    "Parallel(n_jobs=num_cores)(delayed(recover_chainid_protein)(traj_note) for traj_note in traj_notes)\n",
    "Parallel(n_jobs=num_cores)(delayed(recover_chainid_system)(traj_note) for traj_note in traj_notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_md_dataframe():    \n",
    "    md_data = pd.DataFrame(columns=list(['MD_name','pH','replicate','traj_time']))\n",
    "    return md_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metadata(md_data= None):   \n",
    "    def append_metadata(traj_note, location = default_location, skip = default_skip, md_data = md_data):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top= location + top_location)\n",
    "        print(\"In \" + traj_note + \", simulation runs \" + str(10 * traj.n_frames) + \" ns.\")\n",
    "        md_name = traj_note[:traj_note.find('pH')-1]\n",
    "        pH = traj_note[traj_note.find('pH')+2:traj_note.find('pH')+4]\n",
    "        md_replicate = traj_note[-1]\n",
    "        for i in range(0,traj.n_frames):\n",
    "            md_data.loc[md_data.shape[0]+1] = [md_name,pH,md_replicate,i]\n",
    "\n",
    "    for traj_note in traj_notes:\n",
    "        append_metadata(traj_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_system_notation(md_data = None):\n",
    "    system_notation = 0\n",
    "    notation = -1\n",
    "    location = default_location\n",
    "    skip= default_skip\n",
    "    notations = []\n",
    "    for traj_note in traj_notes:\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top= location + top_location)\n",
    "        if traj_note.find('md1') >= 0:\n",
    "            notation = notation + 1\n",
    "        #outliers\n",
    "        if traj_note == '5NJY_I233T_pH70_md4' or traj_note == '4NPQ_pH70_md5' or  traj_note == '4NPQ_F238L_pH70_md3' or traj_note == '4NPQ_I233T_pH70_md3' or traj_note == '4NPQ_F238LI233T_pH70_md3' or traj_note == '4HFI_F238A_pH46_md3':\n",
    "            notation = notation + 1\n",
    "\n",
    "        for frame in range(0,traj.n_frames):\n",
    "            notations.append(notation)\n",
    "        #if increment % 3 == 2:\n",
    "        #    notation = notation + 1\n",
    "        #increment = increment + 1 \n",
    "        \n",
    "    md_data['system'] = notations\n",
    "    print('In total,' + str(notation+1) + ' systems.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In 5NJY_pH70_md1, simulation runs 1360 ns.\n",
      "In 5NJY_pH70_md2, simulation runs 750 ns.\n",
      "In 5NJY_pH70_md3, simulation runs 810 ns.\n",
      "In 5NJY_F238L_pH70_md1, simulation runs 850 ns.\n",
      "In 5NJY_F238L_pH70_md2, simulation runs 1220 ns.\n",
      "In 5NJY_F238L_pH70_md3, simulation runs 720 ns.\n",
      "In 5NJY_I233T_pH70_md4, simulation runs 860 ns.\n",
      "In 5NJY_I233T_pH70_md2, simulation runs 910 ns.\n",
      "In 5NJY_I233T_pH70_md3, simulation runs 820 ns.\n",
      "In 5NJY_F238LI233T_pH70_md1, simulation runs 830 ns.\n",
      "In 5NJY_F238LI233T_pH70_md2, simulation runs 800 ns.\n",
      "In 5NJY_F238LI233T_pH70_md3, simulation runs 880 ns.\n",
      "In 5NJY_pH46_md1, simulation runs 1080 ns.\n",
      "In 5NJY_pH46_md2, simulation runs 1060 ns.\n",
      "In 5NJY_pH46_md3, simulation runs 1030 ns.\n",
      "In 5NJY_F238L_pH46_md1, simulation runs 930 ns.\n",
      "In 5NJY_F238L_pH46_md2, simulation runs 950 ns.\n",
      "In 5NJY_F238L_pH46_md3, simulation runs 1090 ns.\n",
      "In 5NJY_I233T_pH46_md1, simulation runs 810 ns.\n",
      "In 5NJY_I233T_pH46_md2, simulation runs 870 ns.\n",
      "In 5NJY_I233T_pH46_md3, simulation runs 980 ns.\n",
      "In 5NJY_F238LI233T_pH46_md1, simulation runs 790 ns.\n",
      "In 5NJY_F238LI233T_pH46_md2, simulation runs 950 ns.\n",
      "In 5NJY_F238LI233T_pH46_md3, simulation runs 960 ns.\n",
      "In 4HFI_pH46_md1, simulation runs 1100 ns.\n",
      "In 4HFI_pH46_md2, simulation runs 1100 ns.\n",
      "In 4HFI_pH46_md3, simulation runs 1070 ns.\n",
      "In 4HFI_F238L_pH46_md1, simulation runs 1040 ns.\n",
      "In 4HFI_F238L_pH46_md2, simulation runs 1010 ns.\n",
      "In 4HFI_F238L_pH46_md3, simulation runs 1040 ns.\n",
      "In 4HFI_I233T_pH46_md1, simulation runs 1030 ns.\n",
      "In 4HFI_I233T_pH46_md2, simulation runs 1030 ns.\n",
      "In 4HFI_I233T_pH46_md3, simulation runs 1030 ns.\n",
      "In 4HFI_F238LI233T_pH46_md1, simulation runs 1060 ns.\n",
      "In 4HFI_F238LI233T_pH46_md2, simulation runs 1010 ns.\n",
      "In 4HFI_F238LI233T_pH46_md3, simulation runs 1120 ns.\n",
      "In 4NPQ_pH70_md5, simulation runs 840 ns.\n",
      "In 4NPQ_pH70_md6, simulation runs 940 ns.\n",
      "In 4NPQ_pH70_md7, simulation runs 1050 ns.\n",
      "In 4NPQ_F238L_pH70_md3, simulation runs 830 ns.\n",
      "In 4NPQ_F238L_pH70_md4, simulation runs 990 ns.\n",
      "In 4NPQ_F238L_pH70_md5, simulation runs 960 ns.\n",
      "In 4NPQ_I233T_pH70_md3, simulation runs 890 ns.\n",
      "In 4NPQ_I233T_pH70_md4, simulation runs 980 ns.\n",
      "In 4NPQ_I233T_pH70_md5, simulation runs 1050 ns.\n",
      "In 4NPQ_F238LI233T_pH70_md3, simulation runs 880 ns.\n",
      "In 4NPQ_F238LI233T_pH70_md4, simulation runs 920 ns.\n",
      "In 4NPQ_F238LI233T_pH70_md5, simulation runs 1010 ns.\n",
      "In 4HFI_F238A_pH46_md1, simulation runs 870 ns.\n",
      "In 4HFI_F238A_pH46_md2, simulation runs 820 ns.\n",
      "In 4HFI_F238A_pH46_md3, simulation runs 880 ns.\n",
      "In 4HFI_F238L_pH46_md4, simulation runs 1510 ns.\n",
      "In total,16systems.\n"
     ]
    }
   ],
   "source": [
    "md_data = create_md_dataframe()\n",
    "create_metadata(md_data)\n",
    "create_system_notation(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_data['replicate'] = md_data['replicate'].apply(int)\n",
    "md_data['traj_time'] = md_data['traj_time'].apply(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_data.to_pickle('glic_activation.pickles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rmsd_data(md_data= None, ref_name = None, select_domain = None):\n",
    "    def append_rmsd_data(traj_note, location = default_location, ref_name = None, skip = default_skip):\n",
    "        residue_selection_1 = \"resSeq 13 to 198\"\n",
    "        residue_selection_2 = \"resSeq 198 to 316\"\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top= location + top_location)\n",
    "        if select_domain == 'TMD':\n",
    "            traj = traj.atom_slice(traj.topology.select(residue_selection_2))\n",
    "        elif select_domain == 'ECD':\n",
    "            traj = traj.atom_slice(traj.topology.select(residue_selection_1))\n",
    "\n",
    "        if ref_name != None:\n",
    "            ref_location = (ensemble_location + ref_name + \"_1.mer.pdb\")\n",
    "        else:\n",
    "            ref_location = location + top_location\n",
    "        ref_traj = md.load(ref_location)\n",
    "        if select_domain == 'TMD':\n",
    "            ref_traj = ref_traj.atom_slice(ref_traj.topology.select(residue_selection_2))\n",
    "        elif select_domain == 'ECD':\n",
    "            ref_traj = ref_traj.atom_slice(ref_traj.topology.select(residue_selection_1))\n",
    "        if traj.n_atoms != ref_traj.n_atoms:\n",
    "            traj = traj.atom_slice(traj.topology.select(residue_selection))\n",
    "        traj.superpose(ref_traj,0)\n",
    "        rmsd_data.extend(list(md.rmsd(traj, ref_traj)*10))\n",
    "    rmsd_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_rmsd_data(traj_note,ref_name = ref_name)\n",
    "    if select_domain == 'TMD':\n",
    "        md_data['tmd_rmsd']= rmsd_data\n",
    "    elif select_domain == 'ECD':\n",
    "        md_data['ecd_rmsd']= rmsd_data\n",
    "    elif ref_name != None:\n",
    "        md_data['rmsd' + str(ref_name)]= rmsd_data\n",
    "    else:\n",
    "        md_data['rmsd']= rmsd_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pca_data(md_data = None):\n",
    "    def ensemble_pca_cartesian():\n",
    "        wholetraj = md.load(ensemble_location + 'new_ensemble.pdb')\n",
    "        wholetraj = wholetraj.atom_slice(wholetraj.topology.select(\"resSeq 13 to 316\"))\n",
    "        wholetraj.superpose(wholetraj,18)\n",
    "        pca = PCA(n_components=5)\n",
    "        reduced_cartesian = pca.fit_transform(wholetraj.xyz.reshape(wholetraj.n_frames,wholetraj.n_atoms *3))\n",
    "        return wholetraj, pca\n",
    "    def append_pca_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top =location + top_location)\n",
    "        topology = traj.topology\n",
    "        traj = traj.atom_slice(topology.select(\"resSeq 13 to 316\"))\n",
    "        traj.superpose(wholetraj,18)\n",
    "        traj_reduced_cartesian = pca.transform(traj.xyz.reshape(traj.n_frames,traj.n_atoms * 3))\n",
    "        pca1.extend(traj_reduced_cartesian.T[0])\n",
    "        pca2.extend(traj_reduced_cartesian.T[1])\n",
    "        pca3.extend(traj_reduced_cartesian.T[2])\n",
    "        pca4.extend(traj_reduced_cartesian.T[3])\n",
    "        pca5.extend(traj_reduced_cartesian.T[4])\n",
    "\n",
    "\n",
    "    pca1 = []\n",
    "    pca2 = []\n",
    "    pca3 = []\n",
    "    pca4 = []\n",
    "    pca5 = []\n",
    "    wholetraj, pca = ensemble_pca_cartesian()\n",
    "    for traj_note in traj_notes:\n",
    "        append_pca_data(traj_note)\n",
    "    md_data['wholepca_pc1'] = pca1\n",
    "    md_data['wholepca_pc2'] = pca2\n",
    "    md_data['wholepca_pc3'] = pca3\n",
    "    md_data['wholepca_pc4'] = pca4\n",
    "    md_data['wholepca_pc5'] = pca5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cppca_data(md_data = None,residue_selection_1 = \"resSeq 13 to 198\", residue_selection_2 = \"resSeq 198 to 316\"):\n",
    "    def combined_ppca_reduced_cartesian(residue_selection_1 = \"resSeq 13 to 198\", residue_selection_2 = \"resSeq 198 to 316\"):\n",
    "        wholetraj = md.load(ensemble_location + 'new_ensemble.pdb')\n",
    "        wholetraj.superpose(wholetraj,18)\n",
    "        topology = wholetraj.topology\n",
    "        wholetraj_sliced_ecd = wholetraj.atom_slice(topology.select(residue_selection_1))\n",
    "        wholetraj_sliced_ecd.superpose(wholetraj_sliced_ecd,18)\n",
    "        wholetraj_sliced_tmd = wholetraj.atom_slice(topology.select(residue_selection_2))\n",
    "        wholetraj_sliced_tmd.superpose(wholetraj_sliced_tmd,18)\n",
    "        ppca_ecd = PCA(n_components=2)\n",
    "        ppca_tmd = PCA(n_components=2)\n",
    "        partial_reduced_cartesian = [ppca_ecd.fit_transform(wholetraj_sliced_ecd.xyz.reshape(wholetraj_sliced_ecd.n_frames, wholetraj_sliced_ecd.n_atoms * 3)), ppca_tmd.fit_transform(wholetraj_sliced_tmd.xyz.reshape(wholetraj_sliced_tmd.n_frames, wholetraj_sliced_tmd.n_atoms * 3))]    \n",
    "        return partial_reduced_cartesian, wholetraj_sliced_ecd, wholetraj_sliced_tmd,ppca_ecd,ppca_tmd\n",
    "    \n",
    "    def append_projection_on_combined_ppca_data(traj_note, location = default_location, skip = default_skip, residue_selection_1 = \"resSeq 13 to 198\", residue_selection_2 = \"resSeq 198 to 316\"):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top= location + top_location)\n",
    "        topology = traj.topology\n",
    "        traj_sliced_ecd = traj.atom_slice(topology.select(residue_selection_1))\n",
    "        traj_sliced_tmd = traj.atom_slice(topology.select(residue_selection_2))\n",
    "\n",
    "        traj_sliced_ecd.superpose(wholetraj_sliced_ecd,18)\n",
    "        traj_sliced_tmd.superpose(wholetraj_sliced_tmd,18)\n",
    "\n",
    "        reduced_cartesian_ecd_pc1.extend(ppca_ecd.transform(traj_sliced_ecd.xyz.reshape(traj_sliced_ecd.n_frames, traj_sliced_ecd.n_atoms * 3)).T[0])\n",
    "        reduced_cartesian_ecd_pc2.extend(ppca_ecd.transform(traj_sliced_ecd.xyz.reshape(traj_sliced_ecd.n_frames, traj_sliced_ecd.n_atoms * 3)).T[1])\n",
    "        reduced_cartesian_tmd_pc1.extend(ppca_tmd.transform(traj_sliced_tmd.xyz.reshape(traj_sliced_tmd.n_frames, traj_sliced_tmd.n_atoms * 3)).T[0])\n",
    "        reduced_cartesian_tmd_pc2.extend(ppca_tmd.transform(traj_sliced_tmd.xyz.reshape(traj_sliced_tmd.n_frames, traj_sliced_tmd.n_atoms * 3)).T[1])\n",
    "\n",
    "    partial_reduced_cartesian, wholetraj_sliced_ecd,wholetraj_sliced_tmd, ppca_ecd,ppca_tmd = combined_ppca_reduced_cartesian(residue_selection_1,residue_selection_2)\n",
    "    \n",
    "    reduced_cartesian_ecd_pc1 = []\n",
    "    reduced_cartesian_ecd_pc2 = []\n",
    "    reduced_cartesian_tmd_pc1 = []\n",
    "    reduced_cartesian_tmd_pc2 = []\n",
    "\n",
    "    for traj_note in traj_notes:\n",
    "        append_projection_on_combined_ppca_data(traj_note)\n",
    "    md_data['ecd_pc1']= reduced_cartesian_ecd_pc1\n",
    "    md_data['ecd_pc2']= reduced_cartesian_ecd_pc2\n",
    "    md_data['tmd_pc1']= reduced_cartesian_tmd_pc1\n",
    "    md_data['tmd_pc2']= reduced_cartesian_tmd_pc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_rmsd_data(md_data)\n",
    "create_pca_data(md_data)\n",
    "create_cppca_data(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "class Clusterer(object):\n",
    "    \n",
    "    def __init__(self, eps):\n",
    "        self.eps = eps\n",
    "        \n",
    "    \n",
    "    def _preprocess(self, feature):\n",
    "        \n",
    "\n",
    "        ss = StandardScaler()\n",
    "        X = ss.fit_transform(feature[['ecd_pc1','tmd_pc1']].values)\n",
    "        \n",
    "        return X\n",
    "    \n",
    "    \n",
    "    def predict(self, feature):\n",
    "        \n",
    "        X = self._preprocess(feature)\n",
    "        \n",
    "        cl = DBSCAN(eps=self.eps, min_samples=1, algorithm='kd_tree')\n",
    "        labels = cl.fit_predict(X)\n",
    "        \n",
    "        return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan 0 means major cluster, -1 means being ruled out.\n",
    "md_data['dbscan'] = 0\n",
    "for system in range(0,8):\n",
    "    md_data.loc[(md_data.system == system) & (md_data.traj_time < 5),'dbscan'] = -1    \n",
    "for system in range(8,18):\n",
    "    model = Clusterer(eps=0.2)\n",
    "    label = model.predict(md_data[md_data.system == system])\n",
    "    md_data.loc[md_data.system == system,'dbscan'] = label\n",
    "    md_data.loc[(md_data.system == system) & (md_data.dbscan != mode(label)),'dbscan'] = -1\n",
    "    md_data.loc[(md_data.system == system) & (md_data.dbscan == mode(label)),'dbscan'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_data.to_pickle('glic_activation.pickles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5\n",
    "Preprocess: run **helix_tilt.sh** & **helix_twist.sh**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_domain_twist_data(md_data = None):\n",
    "    def append_domain_twist(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top= location + top_location)        \n",
    "        traj.superpose(traj,0)\n",
    "        topology = traj.topology\n",
    "        domain_twist = []\n",
    "        for chain in range (0,5):\n",
    "            residue_selection_1 = \"resid \" + str(8+chain*311) + \" to \" + str(192+chain*311)\n",
    "            residue_selection_2 = \"resid \" + str(192+chain*311) + \" to \" + str(310+chain*311)\n",
    "            traj_sliced_ecd = traj.atom_slice(topology.select(residue_selection_1))\n",
    "            traj_sliced_tmd = traj.atom_slice(topology.select(residue_selection_2))\n",
    "            angle = []\n",
    "            for i in range(0,traj.n_frames):\n",
    "                cen_mass_ecd = md.compute_center_of_mass(traj_sliced_ecd[i])[0]\n",
    "                cen_mass_tmd = md.compute_center_of_mass(traj_sliced_tmd[i])[0]\n",
    "                cen_mass = md.compute_center_of_mass(traj[i])[0]\n",
    "                cen_mass_ecd[2] = cen_mass[2]\n",
    "                cen_mass_tmd[2] = cen_mass[2]\n",
    "                vec_ecd = cen_mass_ecd - cen_mass\n",
    "                vec_tmd = cen_mass_tmd - cen_mass\n",
    "                veclength_ecd = np.sqrt(np.sum(np.power(vec_ecd,2)))\n",
    "                veclength_tmd = np.sqrt(np.sum(np.power(vec_tmd,2)))\n",
    "                angle.append(57.2958 * np.arccos(np.dot(vec_ecd,vec_tmd) /(veclength_ecd * veclength_tmd)))\n",
    "            domain_twist.append(angle)\n",
    "        domain_twist_data.extend(np.mean(np.asarray(domain_twist),axis=0))\n",
    "    domain_twist_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_domain_twist(traj_note)\n",
    "    md_data['domain twist']= domain_twist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_helix_tilt_data(md_data = None):\n",
    "    def append_helix_tilt_data(traj_note, location = default_location, skip = default_skip):\n",
    "        tilt_data = pd.read_csv(location + traj_note + '/analysis/' + traj_note + '.tilt.csv',sep=\" \")\n",
    "        tilt_data.columns = ['traj_time','avg','ang1','ang2','ang3','ang4','ang5']\n",
    "        helix_tilt_data.extend(tilt_data['avg'])\n",
    "    helix_tilt_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_helix_tilt_data(traj_note)\n",
    "    md_data['helix tilt angle'] = helix_tilt_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_helix_twist_data(md_data = None):\n",
    "    def append_helix_twist_data(traj_note, location = default_location, skip = default_skip):\n",
    "        twist = pd.read_csv(location + traj_note + '/analysis/' + traj_note + '.twist.csv',sep=\" \")\n",
    "        twist.columns = ['traj_time','avg','ang1','ang2','ang3','ang4','ang5']\n",
    "        helix_twist_data.extend(twist['avg'])\n",
    "    helix_twist_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_helix_twist_data(traj_note)\n",
    "    md_data['helix twist angle'] = helix_twist_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_domain_twist_data(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_helix_tilt_data(md_data)\n",
    "create_helix_twist_data(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_beta_expansion(md_data = None):\n",
    "    def append_beta_expansion_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        distance = []\n",
    "        for chain in range(0,5):\n",
    "            distance.append(md.compute_distances(traj,[[27 + chain * 311,187 + chain * 311]]))\n",
    "        beta_expansion_data.extend(np.mean(distance,axis=0).T[0])\n",
    "    beta_expansion_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_beta_expansion_data(traj_note)\n",
    "    md_data['beta_expansion'] = beta_expansion_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_M2_radius(md_data = None):\n",
    "    def append_M2_radius_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        topology = traj.topology\n",
    "        M2_selection = 'resSeq 231 to 245'\n",
    "        traj_M2 = traj.atom_slice(topology.select(M2_selection))\n",
    "        M2_radius_data.extend(md.compute_rg(traj_M2).T) \n",
    "    M2_radius_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_M2_radius_data(traj_note)\n",
    "    md_data['M2_radius'] = M2_radius_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ECD_radius(md_data = None):\n",
    "    def append_ECD_radius_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        topology = traj.topology\n",
    "        ECD_selection = 'resSeq 5 to 194'\n",
    "        traj_ECD = traj.atom_slice(topology.select(ECD_selection))\n",
    "        ECD_radius_data.extend(md.compute_rg(traj_ECD).T) \n",
    "    ECD_radius_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_ECD_radius_data(traj_note)\n",
    "    md_data['ECD_radius'] = ECD_radius_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_M2_M1_distance(md_data = None):\n",
    "    def distance_calculate(x,y):\n",
    "        dist = np.sqrt(np.power(x[0]-y[0],2) + np.power(x[1]-y[1],2) + np.power(x[2]-y[2],2))\n",
    "        return dist\n",
    "    def append_M2_M1_distance_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        topology = traj.topology\n",
    "        distance = []\n",
    "        for chain in range(0,4):\n",
    "            M1_selection = 'resid ' + str(192 + chain * 311) + ' to ' + str(196 + chain * 311)\n",
    "            M2_selection = 'resid ' + str(233 + (chain + 1) * 311) + ' to ' + str(238 + (chain + 1) * 311)\n",
    "            traj_M1 = traj.atom_slice(topology.select(M1_selection))\n",
    "            traj_M2 = traj.atom_slice(topology.select(M2_selection))\n",
    "            distance.append(distance_calculate(md.compute_center_of_mass(traj_M1).T,md.compute_center_of_mass(traj_M2).T))\n",
    "        M1_selection = 'resid ' + str(192 + 4 * 311) + ' to ' + str(196 + 4 * 311)\n",
    "        M2_selection = 'resid ' + str(233) + ' to ' + str(238)\n",
    "        traj_M1 = traj.atom_slice(topology.select(M1_selection))\n",
    "        traj_M2 = traj.atom_slice(topology.select(M2_selection))\n",
    "        distance.append(distance_calculate(md.compute_center_of_mass(traj_M1).T,md.compute_center_of_mass(traj_M2).T))\n",
    "        #print((np.mean(np.asarray(distance),axis=0))\n",
    "        M2_M1_distance.extend(np.mean(distance,axis=0))\n",
    "    M2_M1_distance = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_M2_M1_distance_data(traj_note)\n",
    "    md_data['M2_M1_distance'] = M2_M1_distance\n",
    "    #print(M2_M1_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_M2_M1_distance_per_chain(md_data = None):\n",
    "    def distance_calculate(x,y):\n",
    "        dist = np.sqrt(np.power(x[0]-y[0],2) + np.power(x[1]-y[1],2) + np.power(x[2]-y[2],2))\n",
    "        return dist\n",
    "    def append_M2_M1_distance_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        topology = traj.topology\n",
    "        distance = []\n",
    "        for chain in range(0,4):\n",
    "            M1_selection = 'resid ' + str(192 + chain * 311) + ' to ' + str(196 + chain * 311)\n",
    "            M2_selection = 'resid ' + str(233 + (chain + 1) * 311) + ' to ' + str(238 + (chain + 1) * 311)\n",
    "            traj_M1 = traj.atom_slice(topology.select(M1_selection))\n",
    "            traj_M2 = traj.atom_slice(topology.select(M2_selection))\n",
    "            distance.append(distance_calculate(md.compute_center_of_mass(traj_M1).T,md.compute_center_of_mass(traj_M2).T))\n",
    "\n",
    "        M1_selection = 'resid ' + str(192 + 4 * 311) + ' to ' + str(196 + 4 * 311)\n",
    "        M2_selection = 'resid ' + str(233) + ' to ' + str(238)\n",
    "        traj_M1 = traj.atom_slice(topology.select(M1_selection))\n",
    "        traj_M2 = traj.atom_slice(topology.select(M2_selection))\n",
    "        distance.append(distance_calculate(md.compute_center_of_mass(traj_M1).T,md.compute_center_of_mass(traj_M2).T))\n",
    "        #print((np.mean(np.asarray(distance),axis=0))\n",
    "        M2_M1_distance_1.extend(distance[0])\n",
    "        M2_M1_distance_2.extend(distance[1])\n",
    "        M2_M1_distance_3.extend(distance[2])\n",
    "        M2_M1_distance_4.extend(distance[3])\n",
    "        M2_M1_distance_5.extend(distance[4])\n",
    "\n",
    "    M2_M1_distance_1 = []\n",
    "    M2_M1_distance_2 = []\n",
    "    M2_M1_distance_3 = []\n",
    "    M2_M1_distance_4 = []\n",
    "    M2_M1_distance_5 = []\n",
    "\n",
    "    for traj_note in traj_notes:\n",
    "        append_M2_M1_distance_data(traj_note)\n",
    "    md_data['M2_M1_distance_1'] = M2_M1_distance_1\n",
    "    md_data['M2_M1_distance_2'] = M2_M1_distance_2\n",
    "    md_data['M2_M1_distance_3'] = M2_M1_distance_3\n",
    "    md_data['M2_M1_distance_4'] = M2_M1_distance_4\n",
    "    md_data['M2_M1_distance_5'] = M2_M1_distance_5\n",
    "\n",
    "    #print(M2_M1_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_M1_kink(md_data = None):\n",
    "    def angle_calculate(x,y,z):\n",
    "        angle_set = []\n",
    "        for i in range(0,x.shape[0]):\n",
    "            angle_set.append(180 - 57.29 * np.arccos(np.dot((x[i]-y[i]),(z[i]-y[i]))/(np.linalg.norm(x[i]-y[i]) * np.linalg.norm(z[i]-y[i]))))\n",
    "        return angle_set\n",
    "    def append_M1_kink_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        topology = traj.topology\n",
    "        angle = []\n",
    "        for chain in range(0,5):\n",
    "            M1_selection_up = 'resid ' + str(192 + chain * 311) + ' to ' + str(196 + chain * 311)\n",
    "            M1_selection_mid = 'resid ' + str(194 + chain * 311) + ' to ' + str(198 + chain * 311)\n",
    "            M1_selection_down = 'resid ' + str(197 + chain * 311) + ' to ' + str(208 + chain * 311)\n",
    "\n",
    "            M1_up = traj.atom_slice(topology.select(M1_selection_up))\n",
    "            M1_mid = traj.atom_slice(topology.select(M1_selection_mid))\n",
    "            M1_down = traj.atom_slice(topology.select(M1_selection_down))\n",
    "            angle.append(angle_calculate(md.compute_center_of_mass(M1_up), md.compute_center_of_mass(M1_mid), md.compute_center_of_mass(M1_down)))\n",
    "        M1_kink.extend(np.mean(angle,axis=0))\n",
    "    M1_kink = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_M1_kink_data(traj_note)\n",
    "    md_data['M1_kink'] = M1_kink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_M2_kink(md_data = None):\n",
    "    def angle_calculate(x,y,z):\n",
    "        angle_set = []\n",
    "        for i in range(0,x.shape[0]):\n",
    "            angle_set.append(180 - 57.29 * np.arccos(np.dot((x[i]-y[i]),(z[i]-y[i]))/(np.linalg.norm(x[i]-y[i]) * np.linalg.norm(z[i]-y[i]))))\n",
    "        return angle_set\n",
    "    def append_M2_kink_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        topology = traj.topology\n",
    "        angle = []\n",
    "        for chain in range(0,5):\n",
    "            M2_selection_up = 'resid ' + str(216 + chain * 311) + ' to ' + str(233 + chain * 311)\n",
    "            M2_selection_mid = 'resid ' + str(233 + chain * 311) + ' to ' + str(233 + chain * 311)\n",
    "            M2_selection_down = 'resid ' + str(233 + chain * 311) + ' to ' + str(240 + chain * 311)\n",
    "\n",
    "            M2_up = traj.atom_slice(topology.select(M2_selection_up))\n",
    "            M2_mid = traj.atom_slice(topology.select(M2_selection_mid))\n",
    "            M2_down = traj.atom_slice(topology.select(M2_selection_down))\n",
    "            angle.append(angle_calculate(md.compute_center_of_mass(M2_up), md.compute_center_of_mass(M2_mid), md.compute_center_of_mass(M2_down)))\n",
    "        M2_kink.extend(np.mean(angle,axis=0))\n",
    "    M2_kink = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_M2_kink_data(traj_note)\n",
    "    md_data['M2_kink'] = M2_kink\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pore_profile(md_data = None):\n",
    "    def pore_rad(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".protein.gro\"\n",
    "        traj_name = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".protein.xtc\"    \n",
    "        traj = md.load(location + traj_name,top = location + top_location)\n",
    "        ca_top_location = traj_note + '/' + traj_note +  \".ca.pdb\"\n",
    "        ca_traj_name = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"    \n",
    "        ca_traj = md.load(location + ca_traj_name,top = location + ca_top_location)\n",
    "        topology = traj.topology\n",
    "        pore_profile = pd.DataFrame(columns=['traj_time','resid','pore_radius'])\n",
    "        m = 0\n",
    "        incre = 0\n",
    "        for i in [217,221,225,228,232,235]:\n",
    "            group_1 = np.arange(i,i + 1245, 311)\n",
    "            xyz_inter = np.mean(ca_traj.xyz[:,group_1],axis=1)\n",
    "            chain = topology.add_chain()\n",
    "            residue = topology.add_residue('ALA',chain)\n",
    "            topology.add_atom('H','H',residue)\n",
    "            traj.topology = topology\n",
    "            traj.xyz = np.append(traj.xyz,xyz_inter.reshape([traj.n_frames,1,3]),axis=1)\n",
    "            pairs = list(itertools.product(group_1,[1555 + incre]))\n",
    "            for j in range(0,traj.n_frames):\n",
    "                pore_profile.loc[m] = [j,i,np.mean(md.compute_contacts(traj[j], pairs)[0])]\n",
    "                m = m + 1\n",
    "            incre = incre +1\n",
    "        pore_data_222.extend(pore_profile[pore_profile['resid'] == 217]['pore_radius'])\n",
    "        pore_data_226.extend(pore_profile[pore_profile['resid'] == 221]['pore_radius'])\n",
    "        pore_data_230.extend(pore_profile[pore_profile['resid'] == 225]['pore_radius'])\n",
    "        pore_data_233.extend(pore_profile[pore_profile['resid'] == 228]['pore_radius'])\n",
    "        pore_data_237.extend(pore_profile[pore_profile['resid'] == 232]['pore_radius'])\n",
    "        pore_data_240.extend(pore_profile[pore_profile['resid'] == 235]['pore_radius'])\n",
    "    import itertools\n",
    "    pore_data_222 = []\n",
    "    pore_data_226 = []\n",
    "    pore_data_230 = []\n",
    "    pore_data_233 = []\n",
    "    pore_data_237 = []\n",
    "    pore_data_240 = []\n",
    "\n",
    "    for traj_note in traj_notes:\n",
    "        pore_rad(traj_note)\n",
    "    md_data['pore_profile_222'] = pore_data_222\n",
    "    md_data['pore_profile_226'] = pore_data_226\n",
    "    md_data['pore_profile_230'] = pore_data_230\n",
    "    md_data['pore_profile_233'] = pore_data_233\n",
    "    md_data['pore_profile_237'] = pore_data_237\n",
    "    md_data['pore_profile_240'] = pore_data_240"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_beta_expansion(md_data)\n",
    "create_M2_radius(md_data)\n",
    "create_ECD_radius(md_data)\n",
    "create_M2_M1_distance(md_data)\n",
    "create_M2_M1_distance_per_chain(md_data)\n",
    "create_M1_kink(md_data)\n",
    "create_M2_kink(md_data)\n",
    "create_pore_profile(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hydration_profile(md_data = None):\n",
    "    def append_hydration_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".system.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "        traj = mda.Universe(location + top_location,location + traj_location)\n",
    "        hydration = traj.select_atoms(\"(cyzone 7 10 -10 resid 235) and name OW\",updating = True)\n",
    "        for i in range(0,traj.trajectory.n_frames):\n",
    "            traj.trajectory[i]\n",
    "            hydration_data.append(hydration.n_atoms)\n",
    "    hydration_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_hydration_data(traj_note)\n",
    "    md_data['hydration_data'] = hydration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hydration_profile_2(md_data = None):\n",
    "    def append_hydration_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".system.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "        traj = mda.Universe(location + top_location,location + traj_location)\n",
    "        hydration = traj.select_atoms(\"(cyzone 7 8 -8 resid 236) and name OW\",updating = True)\n",
    "        for i in range(0,traj.trajectory.n_frames):\n",
    "            traj.trajectory[i]\n",
    "            hydration_data.append(hydration.n_atoms)\n",
    "    hydration_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_hydration_data(traj_note)\n",
    "    md_data['hydration_data_236_center'] = hydration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hydration_profile_resid(md_data = None):\n",
    "    def append_hydration_data(traj_note,resid, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".system.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "        traj = mda.Universe(location + top_location,location + traj_location)\n",
    "        hydration = traj.select_atoms(\"(cyzone 7 3 -3 resid \" + str(resid) + \") and name OW\",updating = True)\n",
    "        for i in range(0,traj.trajectory.n_frames):\n",
    "            traj.trajectory[i]\n",
    "            hydration_data.append(hydration.n_atoms)\n",
    "    for resid in [235,233,238,226,240]:\n",
    "        hydration_data = []\n",
    "        for traj_note in traj_notes:\n",
    "            append_hydration_data(traj_note,resid)\n",
    "        md_data['hydration_data' + str(resid) + '_3a'] = hydration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hydration_profile(md_data)\n",
    "create_hydration_profile_2(md_data)\n",
    "create_hydration_profile_resid(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hydration_profile_non_pore(md_data = None):\n",
    "    def append_hydration_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".system.pdb\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "        traj = mda.Universe(location + top_location,location + traj_location)\n",
    "        hydration = traj.select_atoms(\"(cylayer 7 20 8 -8 resid 235) and resname HOH and ((type O))\",updating = True)\n",
    "        for i in range(0,traj.trajectory.n_frames):\n",
    "            traj.trajectory[i]\n",
    "            hydration_data.append(hydration.n_atoms)\n",
    "    hydration_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_hydration_data(traj_note)\n",
    "    md_data['hydration_data_non_pore'] = hydration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hydration_profile_non_pore(md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hydration_profile_non_pore_inter(md_data = None):\n",
    "    def append_hydration_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".system3.pdb\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "        traj = mda.Universe(location + top_location,location + traj_location)\n",
    "        hydration = traj.select_atoms(\"(((cyzone 10 10 -10 resnum 230) and (cyzone 10 10 -10 resnum 541)) or ((cyzone 10 10 -10 resnum 541) and (cyzone 10 10 -10 resnum 852)) or ((cyzone 10 10 -10 resnum 852) and (cyzone 10 10 -10 resnum 1163)) or((cyzone 10 10 -10 resnum 1163) and (cyzone 10 10 -10 resnum 1474)) or((cyzone 10 10 -10 resnum 1474) and (cyzone 10 10 -10 resnum 230))) and (cylayer 7 20 8 -8 resid 235) and resname HOH and ((type O))\",updating = True)\n",
    "        for i in range(0,traj.trajectory.n_frames):\n",
    "            traj.trajectory[i]\n",
    "            hydration_data.append(hydration.n_atoms)\n",
    "    hydration_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_hydration_data(traj_note)\n",
    "    md_data['hydration_data_inter_new'] = hydration_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hydration_profile_non_pore_intra(md_data = None):\n",
    "    def append_hydration_data(traj_note, location = default_location, skip = default_skip):\n",
    "        top_location = traj_note + '/' + traj_note + \".system3.pdb\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "        traj = mda.Universe(location + top_location,location + traj_location)\n",
    "        hydration = traj.select_atoms(\"(not (((cyzone 10 10 -10 resnum 230) and (cyzone 10 10 -10 resnum 541)) or ((cyzone 10 10 -10 resnum 541) and (cyzone 10 10 -10 resnum 852)) or ((cyzone 10 10 -10 resnum 852) and (cyzone 10 10 -10 resnum 1163)) or ((cyzone 10 10 -10 resnum 1163) and (cyzone 10 10 -10 resnum 1474)) or ((cyzone 10 10 -10 resnum 1474) and (cyzone 10 10 -10 resnum 230)))) and (cylayer 7 20 8 -8 resid 235) and resname HOH and ((type O))\",updating = True)\n",
    "        for i in range(0,traj.trajectory.n_frames):\n",
    "            traj.trajectory[i]\n",
    "            hydration_data.append(hydration.n_atoms)\n",
    "    hydration_data = []\n",
    "    for traj_note in traj_notes:\n",
    "        append_hydration_data(traj_note)\n",
    "    md_data['hydration_data_intra_new'] = hydration_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6\n",
    "preprocess: run **hbond_tmd.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hond_analysis_folder(traj_note, location = default_location, skip = default_skip):   \n",
    "    import os, sys\n",
    "    path = location + traj_note + '/hbond_analysis'\n",
    "    os.mkdir(path)\n",
    "for traj_note in traj_notes:\n",
    "    append_hydration_data(traj_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Hbond_data(traj_note, location = default_location, skip = default_skip):\n",
    "    hbond_data = pd.read_csv(location + traj_note + '/hbond_tmd.csv')\n",
    "    water_bridge_data = pd.read_csv(location + traj_note + '/water_bridge_tmd.csv')\n",
    "    top_location = traj_note + '/' + traj_note + \".system.pdb\"\n",
    "    traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "    traj = md.load(location + traj_location,top = location + top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    n_frames = traj.n_frames\n",
    "    for resid in range(198,315):\n",
    "        hbond_dataframe = pd.DataFrame(columns=list(['traj_time','resid','counterpart']))\n",
    "        water_bridge_dataframe = pd.DataFrame(columns=list(['traj_time','resid','counterpart']))   \n",
    "        for time in range(0,n_frames*10000,10000):  \n",
    "            \n",
    "            # Gather acceptor_list donor_list data\n",
    "            \n",
    "            hbond_donor = hbond_data[(hbond_data['time'] == time ) & (hbond_data['donor_resid'] == resid)]\n",
    "            n_donor_hbond = len(hbond_donor)\n",
    "            acceptor_list = []\n",
    "            for donor_ind in sorted(set(hbond_donor['donor_index'])):\n",
    "                acceptor_list.append([donor_ind,hbond_donor[hbond_donor['donor_index'] == donor_ind].iloc[:,3:4].values.T.tolist()[0]])\n",
    "                \n",
    "            hbond_acceptor = hbond_data[(hbond_data['time'] == time) & (hbond_data['acceptor_resid'] == resid)]\n",
    "            n_acceptor_hbond = len(hbond_acceptor)\n",
    "            donor_list = []\n",
    "            for acceptor_ind in sorted(set(hbond_acceptor['acceptor_index'])):\n",
    "                donor_list.append([acceptor_ind,hbond_acceptor[hbond_acceptor['acceptor_index'] == acceptor_ind].iloc[:,2:3].values.T.tolist()[0]])\n",
    "                \n",
    "            for hbond_pair in donor_list + acceptor_list:\n",
    "                if (table.loc[hbond_pair[0]].resName != 'HOH') & (table.loc[hbond_pair[0]].resName != 'CL') & (table.loc[hbond_pair[0]].resName != 'NA') & (table.loc[hbond_pair[0]].resName != 'POP'):\n",
    "                    hbond_counterparts = []\n",
    "                    for hbond_counterpart in hbond_pair[1]:\n",
    "                        if (table.loc[hbond_counterpart].resName != 'HOH') & (table.loc[hbond_counterpart].resName != 'CL') & (table.loc[hbond_counterpart].resName != 'NA') & (table.loc[hbond_counterpart].resName != 'POP'):\n",
    "                            hbond_counterparts.append(hbond_counterpart)\n",
    "                    hbond_dataframe = hbond_dataframe.append({'traj_time':time,'resid': hbond_pair[0], 'counterpart': hbond_counterparts},ignore_index=True)\n",
    "                \n",
    "            # Analyze percentage\n",
    "           \n",
    "            water_bridge_donor = water_bridge_data[(water_bridge_data['time'] == time) & (water_bridge_data['donor_resid'] == resid)]\n",
    "            acceptor_list = []\n",
    "            if not(water_bridge_donor.empty):\n",
    "                for donor_ind in sorted(set(water_bridge_donor['donor_index'])):\n",
    "                    for water_resid in set(water_bridge_donor[water_bridge_donor['donor_index'] == donor_ind]['acceptor_resid']):\n",
    "                        acceptor_list.append([donor_ind,water_bridge_data[(water_bridge_data['time'] == time) & (water_bridge_data['acceptor_resid'] == water_resid) & (water_bridge_data['donor_resid'] != resid)].iloc[:,2:3].values.flatten().tolist()])\n",
    "                        acceptor_list.append([donor_ind,water_bridge_data[(water_bridge_data['time'] == time) & (water_bridge_data['donor_resid'] == water_resid) & (water_bridge_data['donor_resid'] != resid)].iloc[:,3:4].values.flatten().tolist()])\n",
    "            water_bridge_acceptor = water_bridge_data[(water_bridge_data['time'] == time) & (water_bridge_data['acceptor_resid'] == resid)]\n",
    "            donor_list = []\n",
    "            if not(water_bridge_acceptor.empty):\n",
    "                for acceptor_ind in sorted(set(water_bridge_donor['acceptor_index'])):\n",
    "                    for water_resid in set(water_bridge_donor[water_bridge_donor['acceptor_index'] == acceptor_ind]['donor_resid']):\n",
    "                        donor_list.append([donor_ind,water_bridge_data[(water_bridge_data['time'] == time) & (water_bridge_data['acceptor_resid'] == water_resid) & (water_bridge_data['acceptor_resid'] != resid)].iloc[:,2:3].values.flatten().tolist()])\n",
    "                        donor_list.append([donor_ind,water_bridge_data[(water_bridge_data['time'] == time) & (water_bridge_data['donor_resid'] == water_resid) & (water_bridge_data['acceptor_resid'] != resid)].iloc[:,3:4].values.flatten().tolist()])\n",
    "            for w_bridge_pair in acceptor_list + donor_list:\n",
    "                if (table.loc[w_bridge_pair[0]].resName != 'HOH') & (table.loc[w_bridge_pair[0]].resName != 'CL') & (table.loc[w_bridge_pair[0]].resName != 'NA') & (table.loc[w_bridge_pair[0]].resName != 'POP'):\n",
    "                    if w_bridge_pair[1] == []:\n",
    "                        water_bridge_dataframe = water_bridge_dataframe.append({'traj_time':time,'resid': w_bridge_pair[0], 'counterpart': [w_bridge_pair[0]]},ignore_index=True)\n",
    "                    else:\n",
    "                        water_bridgecounterparts = []\n",
    "                        for water_bridge_counterpart in w_bridge_pair[1]:\n",
    "                            if (table.loc[water_bridge_counterpart].resName != 'HOH') & (table.loc[water_bridge_counterpart].resName != 'CL') & (table.loc[water_bridge_counterpart].resName != 'NA') & (table.loc[water_bridge_counterpart].resName != 'POP'):\n",
    "                                water_bridgecounterparts.append(water_bridge_counterpart)\n",
    "                            water_bridge_dataframe = water_bridge_dataframe.append({'traj_time':time,'resid': w_bridge_pair[0], 'counterpart': water_bridgecounterparts},ignore_index=True)\n",
    "\n",
    "\n",
    "        hbond_dataframe.to_csv(location + traj_note + '/hbond_analysis/' + str(resid) + '_hbond_list.csv' )\n",
    "        water_bridge_dataframe.to_csv(location + traj_note + '/hbond_analysis/' + str(resid) + '_water_bridge_list.csv')\n",
    "    print('finishing!')\n",
    "   #         print('At time ' + str(time) + ', in total ' + str(n_hbond) + ' hydrogen bonds between residues, resid '\n",
    "   #               + str(resid) + ' forms ' + str(n_intra_hbond) + ' intra-subunit H-bond and ' + str(n_inter_hbond) + ' inter-subunit H-bond.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_pair_hbond_dataframe = pd.DataFrame(columns=list(['traj_note','traj_time','resid1','resid2','inter_hbond','intra_hbond','inter_water_bridge','intra_water_bridge']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resid_pair_hbond_dataframe.to_pickle('pair_hbond.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_resid_pair_hbond_data(traj_note, location = default_location, skip = default_skip):\n",
    "    top_location = traj_note + '/' + traj_note + \".system.pdb\"\n",
    "    traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "    traj = md.load(location + traj_location,top = location + top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    n_frames = traj.n_frames\n",
    "    resid_pair_hbond_dataframe = pd.DataFrame(columns=list(['traj_note','traj_time','resid1','resid2','inter_hbond','intra_hbond','inter_water_bridge','intra_water_bridge']))\n",
    "#        resid_pair_hbond_dataframe = pd.read_pickle('pair_hbond.pickle')\n",
    "    for resid1 in np.arange(198,315):\n",
    "        hbond_dataframe = pd.read_csv(location + traj_note + '/hbond_analysis/' + str(resid1) + '_hbond_list.csv')\n",
    "        import ast\n",
    "        hbond_dataframe['counterpart'] = hbond_dataframe['counterpart'].apply(lambda x: ast.literal_eval(x))\n",
    "        water_bridge_dataframe = pd.read_csv(location + traj_note + '/hbond_analysis/' + str(resid1) + '_water_bridge_list.csv')\n",
    "        water_bridge_dataframe['counterpart'] = water_bridge_dataframe['counterpart'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "        for time in range(0,n_frames*10000,10000): \n",
    "\n",
    "\n",
    "            for residue_id in set(hbond_dataframe['resid']):\n",
    "\n",
    "                counterpart_residue = hbond_dataframe[(hbond_dataframe['resid'] == residue_id) & (hbond_dataframe['traj_time'] == time)]['counterpart']\n",
    "                for counterpart_residue_list in counterpart_residue:\n",
    "                    for counterpart_residue_id in counterpart_residue_list:\n",
    "                        if table.loc[counterpart_residue_id].chainID != table.loc[residue_id].chainID:\n",
    "                            resid_pair_hbond_dataframe = resid_pair_hbond_dataframe.append({'traj_note': traj_note, 'traj_time': time/10000,'resid1': resid1, 'resid2':table.loc[counterpart_residue_id].resSeq, 'inter_hbond': table.loc[counterpart_residue_id].resSeq},ignore_index=True)\n",
    "\n",
    "                        else:\n",
    "                            resid_pair_hbond_dataframe = resid_pair_hbond_dataframe.append({'traj_note': traj_note, 'traj_time': time/10000,'resid1': resid1, 'resid2':table.loc[counterpart_residue_id].resSeq, 'intra_hbond': table.loc[counterpart_residue_id].resSeq},ignore_index=True)\n",
    "\n",
    "\n",
    "            for residue_id in set(water_bridge_dataframe['resid']):\n",
    "                counterpart_residue = water_bridge_dataframe[(water_bridge_dataframe['resid'] == residue_id) & (water_bridge_dataframe['traj_time'] == time)]['counterpart'].values\n",
    "                for counterpart_residue_list in counterpart_residue:\n",
    "                    for counterpart_residue_id in counterpart_residue_list:\n",
    "                        if table.loc[counterpart_residue_id].chainID != table.loc[residue_id].chainID:\n",
    "                            resid_pair_hbond_dataframe = resid_pair_hbond_dataframe.append({'traj_note': traj_note, 'traj_time': time/10000,'resid1': resid1, 'resid2':table.loc[counterpart_residue_id].resSeq, 'inter_water_bridge': table.loc[counterpart_residue_id].resSeq},ignore_index=True)\n",
    "                            resid_pair_hbond_dataframe = resid_pair_hbond_dataframe.append({'traj_note': traj_note, 'traj_time': time/10000,'resid1': table.loc[counterpart_residue_id].resSeq, 'resid2':resid1, 'inter_water_bridge': resid1},ignore_index=True)\n",
    "\n",
    "                        else:\n",
    "                            resid_pair_hbond_dataframe = resid_pair_hbond_dataframe.append({'traj_note': traj_note, 'traj_time': time/10000,'resid1': resid1, 'resid2':table.loc[counterpart_residue_id].resSeq, 'intra_water_bridge': table.loc[counterpart_residue_id].resSeq},ignore_index=True)\n",
    "                            resid_pair_hbond_dataframe = resid_pair_hbond_dataframe.append({'traj_note': traj_note, 'traj_time': time/10000,'resid1': table.loc[counterpart_residue_id].resSeq, 'resid2':resid1, 'intra_water_bridge': resid1},ignore_index=True)\n",
    "\n",
    "    resid_pair_hbond_dataframe.to_pickle('hbond_pair/' + traj_note + '_pair_hbond.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_hbond = pd.DataFrame()\n",
    "for traj_note in traj_notes:\n",
    "    pair_hbond = pair_hbond.append(pd.read_pickle('hbond_pair/' + traj_note + '_pair_hbond.pickle'))\n",
    "pair_hbond.to_pickle('pair_hbond_all.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hbond_data(resid1,resid2,hbond_type,md_data):\n",
    "    interaction_list = []\n",
    "    interaction_df = pair_hbond[(pair_hbond.resid1 == resid1) & (pair_hbond.resid2 == resid2) & (pair_hbond[hbond_type] > 0 )]\n",
    "    for traj_note in traj_notes:\n",
    "        location = default_location\n",
    "        top_location = traj_note + '/' + traj_note + \".ca.gro\"\n",
    "        traj_location = traj_note + '/' + traj_note + \".skip10.ca.xtc\"\n",
    "        traj = md.load(location + traj_location,top = location + top_location)\n",
    "        n_frames = traj.n_frames\n",
    "        for traj_time in range(0,n_frames):\n",
    "            interaction_list.append(interaction_df[(interaction_df.traj_note == traj_note) & (interaction_df.traj_time == traj_time)].shape[0])\n",
    "    md_data[hbond_type + '_' + str(resid1) + '_' + str(resid2)] = interaction_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(200,239,'inter_water_bridge',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(200,243,'inter_water_bridge',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(200,263,'inter_water_bridge',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(200,243,'inter_hbond',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(200,239,'inter_hbond',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(200,234,'intra_hbond',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_hbond_data(205,235,'intra_hbond',md_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Hbond_network_analysis_data(traj_note, location = default_location, skip=10):\n",
    "    top_location = traj_note + '/' + traj_note + \".system.pdb\"\n",
    "    traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".system.xtc\"\n",
    "    traj = md.load(location + traj_location,top = location + top_location)\n",
    "    topology = traj.topology\n",
    "    table, bonds = topology.to_dataframe()\n",
    "    n_frames = traj.n_frames\n",
    "    zero_data = np.zeros(shape=([118*5,118*5]))\n",
    "    hbond_network_dataframe = pd.DataFrame()\n",
    "    for time in range(0,n_frames*10000,10000):  \n",
    "        hbond_network = pd.DataFrame(zero_data,columns=np.asarray([np.arange(193,311),np.arange(193,311) + 311,np.arange(193,311) + 311 * 2,np.arange(193,311) + 311 * 3,np.arange(193,311) + 311 * 4]).flatten())\n",
    "        hbond_network['traj_time'] = time/10000\n",
    "        hbond_network['residue'] = np.asarray([np.arange(193,311),np.arange(193,311) + 311,np.arange(193,311) + 311 * 2,np.arange(193,311) + 311 * 3,np.arange(193,311) + 311 * 4]).flatten()\n",
    "        hbond_network_dataframe = hbond_network_dataframe.append(hbond_network,ignore_index=True)\n",
    "    for time in range(0,n_frames*10000,10000):  \n",
    "        for resid in range(198,315):\n",
    "            hbond_dataframe = pd.read_csv(location + traj_note + '/hbond_analysis/' + str(resid) + '_hbond_list.csv')\n",
    "            hbond_dataframe['counterpart'] = hbond_dataframe['counterpart'].apply(lambda x: ast.literal_eval(x))\n",
    "            water_bridge_dataframe = pd.read_csv(location + traj_note + '/hbond_analysis/' + str(resid) + '_water_bridge_list.csv')\n",
    "            water_bridge_dataframe['counterpart'] = water_bridge_dataframe['counterpart'].apply(lambda x: ast.literal_eval(x))\n",
    "\n",
    "            for residue_id in set(hbond_dataframe['resid']):\n",
    "                counterpart_residue = hbond_dataframe[(hbond_dataframe['resid'] == residue_id) & (hbond_dataframe['traj_time'] == time)]['counterpart']\n",
    "                for counterpart_residue_list in counterpart_residue:\n",
    "                    for counterpart_residue_id in counterpart_residue_list:\n",
    "                        if (table.loc[counterpart_residue_id].resSeq >= 198) & (table.loc[counterpart_residue_id].resSeq <= 315) & (table.loc[residue_id].resSeq >= 198) & (table.loc[residue_id].resSeq <= 315):\n",
    "                            residue1 = table.loc[residue_id].resSeq + (table.loc[residue_id].chainID) * 311 - 5\n",
    "                            residue2 = table.loc[counterpart_residue_id].resSeq + (table.loc[counterpart_residue_id].chainID) * 311 - 5\n",
    "\n",
    "                            hbond_network_dataframe.loc[(hbond_network_dataframe.traj_time == time/10000) & (hbond_network_dataframe.residue == residue1),residue2] = 1\n",
    "                            hbond_network_dataframe.loc[(hbond_network_dataframe.traj_time == time/10000) & (hbond_network_dataframe.residue == residue2),residue1] = 1\n",
    "\n",
    "            for residue_id in set(water_bridge_dataframe['resid']):\n",
    "                counterpart_residue = water_bridge_dataframe[(water_bridge_dataframe['resid'] == residue_id) & (water_bridge_dataframe['traj_time'] == time)]['counterpart'].values\n",
    "                for counterpart_residue_list in counterpart_residue:\n",
    "                    for counterpart_residue_id in counterpart_residue_list:\n",
    "                        if (table.loc[counterpart_residue_id].resSeq >= 198) & (table.loc[counterpart_residue_id].resSeq <= 315) & (table.loc[residue_id].resSeq >= 198) & (table.loc[residue_id].resSeq <= 315):\n",
    "                            residue1 = table.loc[residue_id].resSeq + (table.loc[residue_id].chainID) * 311 - 5\n",
    "                            residue2 = table.loc[counterpart_residue_id].resSeq + (table.loc[counterpart_residue_id].chainID) * 311 - 5\n",
    "                            hbond_network_dataframe.loc[(hbond_network_dataframe.traj_time == time /10000) & (hbond_network_dataframe.residue == residue1),residue2] = 1\n",
    "                            hbond_network_dataframe.loc[(hbond_network_dataframe.traj_time == time /10000) & (hbond_network_dataframe.residue == residue2),residue1] = 1\n",
    "\n",
    "    hbond_network_dataframe.to_csv(location + traj_note + '/hbond_analysis/hbond_network.csv')\n",
    "    print('finishing!')\n",
    "   #         print('At time ' + str(time) + ', in total ' + str(n_hbond) + ' hydrogen bonds between residues, resid '\n",
    "   #               + str(resid) + ' forms ' + str(n_intra_hbond) + ' intra-subunit H-bond and ' + str(n_inter_hbond) + ' inter-subunit H-bond.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hbond_contact_map(traj_note, location = default_location, skip=10):\n",
    "    top_location = traj_note + '/' + traj_note + \".ca.pdb\"\n",
    "    traj_location = traj_note + '/' + traj_note + \".skip\" + str(skip) + \".ca.xtc\"\n",
    "    traj = md.load(location + traj_location,top = location + top_location)\n",
    "    n_frames = traj.n_frames\n",
    "    hbond_contact_map = pd.DataFrame(columns=['residue1','residue2','interaction'])\n",
    "    hbond_network_dataframe = pd.read_csv(location + traj_note + '/hbond_analysis/hbond_network.csv')\n",
    "    for column in hbond_network_dataframe.columns:\n",
    "        if (column !=  'traj_time') & (column != 'residue') & (column != 'Unnamed: 0'):\n",
    "            residue1s = hbond_network_dataframe.loc[(hbond_network_dataframe[column] != 0)]['residue'].values\n",
    "            residue2 = int(column)\n",
    "            for residue1 in set(residue1s):\n",
    "                if hbond_contact_map[(hbond_contact_map.residue1 == residue1) & (hbond_contact_map.residue2 == residue2)].empty:\n",
    "                    interaction_sum = hbond_network_dataframe[(hbond_network_dataframe[column] != 0) & (hbond_network_dataframe.residue == residue1)].shape[0]\n",
    "                    hbond_contact_map = pd.concat([hbond_contact_map,pd.DataFrame([[residue1,residue2,interaction_sum / n_frames]],columns=['residue1','residue2','interaction'])],ignore_index=True)\n",
    "                else:\n",
    "                    interaction_sum = hbond_network_dataframe[(hbond_network_dataframe[column] != 0) & (hbond_network_dataframe.residue == residue1)].shape[0]\n",
    "                    hbond_contact_map.loc[(hbond_contact_map.residue1 == residue1) & (hbond_contact_map.residue2 == residue2),'interaction'] += interaction_sum / n_frames\n",
    "    #print(hbond_contact_map)\n",
    "    hbond_contact_map.to_csv(location + traj_note + '/hbond_analysis/hbond_contact_map.csv')\n",
    "    print('finishing! ' + traj_note)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_ind = np.asarray([np.arange(193,311),np.arange(504,622),np.arange(815,933),np.arange(1126,1244),np.arange(1437,1555)]).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_hbond_network_data(traj_notes):\n",
    "    hbond_data_trans =[]\n",
    "    for traj_note in traj_notes:\n",
    "        hbond_data = pd.read_csv(default_location + traj_note + '/hbond_analysis/hbond_contact_map.csv')\n",
    "        hbond_data_all = pd.DataFrame(columns=['residue1','residue2','hbond'])\n",
    "        for ind in trans_ind:\n",
    "            hbond_data_all = hbond_data_all.append(pd.DataFrame(np.concatenate(([np.zeros([590]) + ind], [trans_ind],[np.zeros([590])]), axis=0).T,columns=['residue1','residue2','hbond']),ignore_index=True)\n",
    "        for residue1,residue2 in hbond_data[['residue1','residue2']].values:\n",
    "            hbond_data_all.loc[(hbond_data_all.residue1 == residue1) & (hbond_data_all.residue2 == residue2),'hbond'] = hbond_data[(hbond_data.residue1 == residue1) & (hbond_data.residue2 == residue2)]['interaction'].values \n",
    "        hbond_data_all.to_csv('/media/scottzhuang/data/MD/' + traj_note + '/hbond_analysis/hbond_contact_map_complete.csv')\n",
    "        hbond_data_trans.append(hbond_data_all['hbond'].values.reshape([590,590]))\n",
    "    hbond_data_mean = np.mean(hbond_data_trans,axis=0)\n",
    "    np.savetxt(default_location + traj_notes[0] + '/hbond_analysis/' + traj_notes[0][:-4] + '_hbond_network_data_transmembrane_mean.dat',hbond_data_mean)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_replicates(traj_notes,location = default_location):\n",
    "    pearson_avg = []\n",
    "    for traj_note in traj_notes:\n",
    "        contact_map = pd.read_csv(location + traj_note + '/hbond_analysis/hbond_contact_map_complete.csv')\n",
    "        pearson_avg.append(contact_map['hbond'].values)\n",
    "    contact_map['hbond'] = np.mean(pearson_avg,axis=0)\n",
    "    return contact_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hbond_map(data,ax,note):\n",
    "    data[data.hbond >= 0.5].plot(kind='scatter',x='residue1',y='residue2',c='hbond',colormap='autumn',ax=ax)\n",
    "    ax.set_title(note + ' Hbond & Water Bridge Map (lifetime > 0.5)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intra_hbond_map(data,note,ax):\n",
    "    data1 = data[(data.residue1 <= 311) & (data.residue2 <= 311)]\n",
    "    data2 = data[(data.residue1 > 311) & (data.residue1 <= 311 *2) &(data.residue2 > 311) & (data.residue2 <= 311 *2)]\n",
    "    data3 = data[(data.residue1 > 311 *2) & (data.residue1 <= 311 *3) &(data.residue2 > 311 *2) & (data.residue2 <= 311 *3)]\n",
    "    data4 = data[(data.residue1 > 311 *3) & (data.residue1 <= 311 *4) &(data.residue2 > 311 *3) & (data.residue2 <= 311 *4)]\n",
    "    data5 = data[(data.residue1 > 311 *4) & (data.residue1 <= 311 *5) &(data.residue2 > 311 *4) & (data.residue2 <= 311 *5)]\n",
    "    data2.residue1 = data1.residue1.values\n",
    "    data2.residue2 = data1.residue2.values\n",
    "    data3.residue1 = data1.residue1.values\n",
    "    data3.residue2 = data1.residue2.values   \n",
    "    data4.residue1 = data1.residue1.values\n",
    "    data4.residue2 = data1.residue2.values   \n",
    "    data5.residue1 = data1.residue1.values\n",
    "    data5.residue2 = data1.residue2.values   \n",
    "    data1 = data1[data1['hbond'] >= 0.1].reset_index()\n",
    "    data2 = data2[data2['hbond'] >= 0.1].reset_index()\n",
    "    data3 = data3[data3['hbond'] >= 0.1].reset_index()\n",
    "    data4 = data4[data4['hbond'] >= 0.1].reset_index()\n",
    "    data5 = data5[data5['hbond'] >= 0.1].reset_index()\n",
    "    data_intra = pd.DataFrame(columns=['residue1','residue2','hbond'])\n",
    "    for pair in np.array(list(set([tuple(t) for t in pd.concat([data1,data2,data3,data4,data5])[['residue1','residue2']].values]))):\n",
    "        data_intra = pd.concat([data_intra,pd.DataFrame([[int(pair[0]) + 5,int(pair[1]) + 5,\n",
    "#                           np.mean([data1[(data1.residue1 == pair[0]) & (data1.residue2 == pair[1])]['hbond'].values,\n",
    "#                                   data2[(data2.residue1 == pair[0]) & (data2.residue2 == pair[1])]['hbond'].values,\n",
    "#                                   data3[(data3.residue1 == pair[0]) & (data3.residue2 == pair[1])]['hbond'].values,\n",
    "#                                   data4[(data4.residue1 == pair[0]) & (data4.residue2 == pair[1])]['hbond'].values,\n",
    "#                                   data5[(data5.residue1 == pair[0]) & (data5.residue2 == pair[1])]['hbond'].values])]],\n",
    "                            np.sum(np.sum([data1.loc[(data1.residue1 == pair[0]) & (data1.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data2.loc[(data2.residue1 == pair[0]) & (data2.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data3.loc[(data3.residue1 == pair[0]) & (data3.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data4.loc[(data4.residue1 == pair[0]) & (data4.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data5.loc[(data5.residue1 == pair[0]) & (data5.residue2 == pair[1]),'hbond'].values.tolist()]))/5]], \n",
    "                                                          columns=['residue1','residue2','hbond']\n",
    "                                  )],ignore_index=True)\n",
    "    data_intra = data_intra.convert_objects(convert_numeric=True)\n",
    "    data_intra.plot(kind='scatter',x='residue1',y='residue2',c=data_intra.hbond,colormap='Greens',ax=ax,colorbar=False,vmax=1,vmin=0)\n",
    "    ax.set_xlim(198,315)\n",
    "    ax.set_ylim(198,315)\n",
    "    data_intra.to_csv(note + '_intra_hbond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inter_hbond_map(data,note,ax):\n",
    "    data1 = data[(data.residue1 > 311 * 0) & (data.residue1 <= 311 * 1) &(data.residue2 > 311 * 1) & (data.residue2 <= 311 *2)]\n",
    "    data2 = data[(data.residue1 > 311 * 1) & (data.residue1 <= 311 * 2) &(data.residue2 > 311 * 2) & (data.residue2 <= 311 *3)]\n",
    "    data3 = data[(data.residue1 > 311 * 2) & (data.residue1 <= 311 * 3) &(data.residue2 > 311 * 3) & (data.residue2 <= 311 *4)]\n",
    "    data4 = data[(data.residue1 > 311 * 3) & (data.residue1 <= 311 * 4) &(data.residue2 > 311 * 4) & (data.residue2 <= 311 *5)]\n",
    "    data5 = data[(data.residue1 > 311 * 4) & (data.residue1 <= 311 * 5) &(data.residue2 > 311 * 0) & (data.residue2 <= 311 * 1)]\n",
    "    data2.residue1 = data1.residue1.values\n",
    "    data2.residue2 = data1.residue2.values\n",
    "    data3.residue1 = data1.residue1.values\n",
    "    data3.residue2 = data1.residue2.values   \n",
    "    data4.residue1 = data1.residue1.values\n",
    "    data4.residue2 = data1.residue2.values   \n",
    "    data5.residue1 = data1.residue1.values\n",
    "    data5.residue2 = data1.residue2.values   \n",
    "    data1 = data1[data1['hbond'] >= 0.1].reset_index()\n",
    "    data2 = data2[data2['hbond'] >= 0.1].reset_index()\n",
    "    data3 = data3[data3['hbond'] >= 0.1].reset_index()\n",
    "    data4 = data4[data4['hbond'] >= 0.1].reset_index()\n",
    "    data5 = data5[data5['hbond'] >= 0.1].reset_index()\n",
    "    data_inter = pd.DataFrame(columns=['residue1','residue2','hbond'])\n",
    "    for pair in np.array(list(set([tuple(t) for t in pd.concat([data1,data2,data3,data4,data5])[['residue1','residue2']].values]))):\n",
    "                            \n",
    "        data_inter = pd.concat([data_inter,pd.DataFrame([[int(pair[0]) + 5,int(pair[1]-311) + 5,\n",
    "                                                        np.sum(np.sum([data1.loc[(data1.residue1 == pair[0]) & (data1.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data2.loc[(data2.residue1 == pair[0]) & (data2.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data3.loc[(data3.residue1 == pair[0]) & (data3.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data4.loc[(data4.residue1 == pair[0]) & (data4.residue2 == pair[1]),'hbond'].values.tolist(),\n",
    "                                                                data5.loc[(data5.residue1 == pair[0]) & (data5.residue2 == pair[1]),'hbond'].values.tolist()]))/5 \n",
    "                                                         ]]\n",
    "                                                        ,columns=['residue1','residue2','hbond'])\n",
    "                                                         ],ignore_index=True)\n",
    "    data_inter = data_inter.convert_objects(convert_numeric=True)\n",
    "    data_inter.plot(kind='scatter',x='residue1',y='residue2',c='hbond',colormap='Greens',ax=ax,colorbar=True,vmax=1,vmin=0)\n",
    "    ax.set_xlim(198,315)\n",
    "    ax.set_ylim(198,315)\n",
    "    data_inter.to_csv(note + '_inter_hbond.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inter_intra_hbond(traj_notes):\n",
    "    fig = plt.figure() \n",
    "    ax1 = fig.add_subplot(1,2,1,aspect='equal') \n",
    "    ax2 = fig.add_subplot(1,2,2,aspect='equal') \n",
    "    plot_intra_hbond_map(gather_replicates(traj_notes),traj_notes[0][:-4],ax1)\n",
    "    plot_inter_hbond_map(gather_replicates(traj_notes),traj_notes[0][:-4],ax2)\n",
    "    plt.suptitle(traj_notes[0][:-4])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interaction_difference(note1,note2,note3):\n",
    "    fig = plt.figure(figsize=(20, 10)) \n",
    "    ax1 = fig.add_subplot(1,2,1,aspect='equal') \n",
    "\n",
    "    ax2 = fig.add_subplot(1,2,2,aspect='equal')\n",
    "\n",
    "    #ax3 = fig.add_subplot(1,3,3) \n",
    "    plot_inter_interaction_difference(note1,note2,note3,ax1)\n",
    "    plot_intra_interaction_difference(note1,note2,note3,ax2)\n",
    "    #plt3 = plot_salt_bridge_interaction(data,ax3)\n",
    "\n",
    "    ax1.set_xlim(198,315)\n",
    "    ax1.set_ylim(198,315)\n",
    "    ax2.set_xlim(198,315)\n",
    "    ax2.set_ylim(198,315) \n",
    "    ax1.set_xticks(np.arange(200,320,10))\n",
    "    ax1.set_yticks(np.arange(200,320,10))\n",
    "    ax2.set_xticks(np.arange(200,320,10))\n",
    "    ax2.set_yticks(np.arange(200,320,10))\n",
    "    ax1.grid()\n",
    "    ax2.grid()\n",
    "    ax1.set_title('inter interaction')\n",
    "    ax2.set_title('intra interaction')\n",
    "    plt.suptitle(note1 + ' vs ' + note2 + ' of ' + note3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(note1 + '_' + note2 + 'diff.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_inter_interaction_difference(note1,note2,note3,ax):\n",
    "    difference = 0.4\n",
    "\n",
    "    interaction_dataframe1 = pd.read_csv(note1 + '_inter_hbond.csv')\n",
    "    interaction_dataframe2 = pd.read_csv(note2 + '_inter_hbond.csv')\n",
    "    for residues in (interaction_dataframe2[['residue1','residue2']].values):\n",
    "        if not(interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])].empty):\n",
    "            interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'] = (interaction_dataframe2[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1])]['hbond'].values - interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])]['hbond'].values)[0]\n",
    "        if ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) >= difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('inter: ' + str(note2) + ' of ' + str(residues))\n",
    "        elif ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) <= -difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('inter: ' + str(note1) + ' of ' + str(residues))\n",
    "    for residues in (interaction_dataframe1[['residue1','residue2']].values):\n",
    "        if interaction_dataframe2[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1])].empty:\n",
    "            interaction_dataframe1.loc[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1]),'hbond'] = - (interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])]['hbond'])\n",
    "            interaction_dataframe2 = interaction_dataframe2.append(interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])],ignore_index = True)\n",
    "        if ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) >= difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('inter: ' + str(note2) + ' of ' + str(residues))\n",
    "        elif ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) <= -difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('inter: ' + str(note1) + ' of ' + str(residues))\n",
    "    interaction_dataframe2.plot(kind='scatter',x='residue1',y='residue2',c=interaction_dataframe2.hbond,colormap='PuOr',ax=ax,colorbar=False,vmin=-0.5, vmax=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_intra_interaction_difference(note1,note2,note3,ax):\n",
    "    difference = 0.4\n",
    "    interaction_dataframe1 = pd.read_csv(note1 + '_intra_hbond.csv')\n",
    "    interaction_dataframe2 = pd.read_csv(note2 + '_intra_hbond.csv')\n",
    "    for residues in (interaction_dataframe2[['residue1','residue2']].values):\n",
    "        if not(interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])].empty):\n",
    "            interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'] = (interaction_dataframe2[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1])]['hbond'].values - interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])]['hbond'].values)[0]\n",
    "        if ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) >= difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('intra: ' + str(note2) + ' of ' + str(residues))\n",
    "        elif ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) <= -difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('intra: ' + str(note1) + ' of ' + str(residues))\n",
    "    for residues in (interaction_dataframe1[['residue1','residue2']].values):\n",
    "        if interaction_dataframe2[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1])].empty:\n",
    "            interaction_dataframe1.loc[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1]),'hbond'] = - (interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])]['hbond'])\n",
    "            interaction_dataframe2 = interaction_dataframe2.append(interaction_dataframe1[(interaction_dataframe1['residue1'] == residues[0]) & (interaction_dataframe1['residue2'] == residues[1])],ignore_index = True)\n",
    "        if ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) >= difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('intra: ' + str(note2) + ' of ' + str(residues))\n",
    "        elif ((interaction_dataframe2.loc[(interaction_dataframe2['residue1'] == residues[0]) & (interaction_dataframe2['residue2'] == residues[1]),'hbond'].values) <= -difference) & (residues[0] > 150) & (residues[1] > 150):\n",
    "            print('intra: ' + str(note1) + ' of ' + str(residues))\n",
    "\n",
    "    interaction_dataframe2.plot(kind='scatter',x='residue1',y='residue2',c=interaction_dataframe2.hbond,colormap='PuOr',ax=ax,colorbar=False,vmin=-0.5, vmax=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interaction_difference('4HFI_pH46','4HFI_F238A_pH46','hbond & water bridge lifetime')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8 Geometric features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = default_location\n",
    "md_traj = md.load(location + '4NPQ_pH70_md5/4NPQ_pH70_md5.skip10.protein.xtc',top =location + '4NPQ_pH70_md5/4NPQ_pH70_md5.protein.gro' )\n",
    "angle_feat = DihedralFeaturizer()\n",
    "angle_feat_describe = pd.DataFrame(angle_feat.describe_features(md_traj))\n",
    "angle_feat_describe = angle_feat_describe.drop(index= angle_feat_describe[angle_feat_describe.resseqs == '[315, 5]'].index)\n",
    "angle_feat_describe.to_pickle('dihedral_angle_feat_describe.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,row in angle_feat_describe.iterrows():\n",
    "    md_data[str(row.featuregroup) + '_' + str(row.otherinfo) + str(row.resseqs)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traj_list = []\n",
    "for traj_note in traj_notes:\n",
    "    traj_list.append(md.load(location + traj_note + '/' + traj_note + '.skip10.protein.xtc',top =location + traj_note + '/' + traj_note + '.protein.gro'))\n",
    "angle_features = angle_feat.fit_transform(traj_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "angle_features = np.delete(np.vstack(angle_features).reshape(857,4,1554),[310, 621, 932, 1243],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_data = md_data.drop(columns = ['phi_sin[315, 5]','phi_cos[315, 5]','psi_sin[315, 5]','psi_cos[315, 5]'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geometric_feature_starting_index = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_data.iloc[:,geometric_feature_starting_index:] = np.mean(angle_features.reshape(857,5,310*4),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1 = 'phi_sin[238, 239]'\n",
    "feature2 = 'phi_sin[236, 237]'\n",
    "fig, axes = plt.subplots(1,1,figsize=(10,10))\n",
    "\n",
    "md_data[md_data.system == 0].plot(kind='scatter',x=feature1,y=feature2,c='#a6611a',ax=axes,s=100)\n",
    "md_data[md_data.system == 1].plot(kind='scatter',x=feature1,y=feature2,c='#80cdc1',ax=axes,s=100)\n",
    "md_data[md_data.system == 2].plot(kind='scatter',x=feature1,y=feature2,c='#dfc27d',ax=axes,s=100)\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "custom_lines = [Line2D([0], [0], marker='o', color='#a6611a', label='Scatter',\n",
    "                        markersize=10,linewidth= 0),\n",
    "                Line2D([0], [0], marker='o', color='#80cdc1', label='Scatter',\n",
    "                           markersize=10,linewidth= 0),\n",
    "                Line2D([0], [0], marker='o', color='#dfc27d', label='Scatter',\n",
    "                          markersize=10,linewidth= 0)]\n",
    "axes.legend(custom_lines, ['WT', 'F238L','F238A'])\n",
    "\n",
    "#plt.savefig('dihedral.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis and plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'md_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-892a1cb38343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmd_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'md_data' is not defined"
     ]
    }
   ],
   "source": [
    "md_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepchem",
   "language": "python",
   "name": "deepchem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
